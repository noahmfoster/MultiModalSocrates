{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Multimodal Learning\n",
    "## Noah Foster\n",
    "\n",
    "First, we will import the necessary libraries and modules, the go through the five steps.\n",
    "\n",
    "These imports are largely cannibalized from the examples. Some code has been modifying to reflect my Macbook Air's lack of a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import clip\n",
    "\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# set random seed:\n",
    "random.seed(2952)\n",
    "np.random.seed(2952)\n",
    "torch.manual_seed(2952)\n",
    "\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.eval()\n",
    "\n",
    "cifar10_test = CIFAR10(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True, train=False)\n",
    "\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar10_test.classes]\n",
    "text_tokens = clip.tokenize(text_descriptions)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: 0-Shot Image Classification (CIFAR 10)\n",
    "## Task 1\n",
    "\n",
    "Finding the image features is the first step. We will use the pretrained ResNet50 model to extract the features from 500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features.shape # (10, 1024) is for the 10 classes and 1024 dimensional embeddings\n",
    "\n",
    "image_indicies = random.sample(range(len(cifar10_test)), 500) # This comes to 1/20th of the test set\n",
    "images = [cifar10_test[idx] for idx in image_indicies] # 500 tuples of (image, label) where image is \n",
    "\n",
    "test_labels = [image[1] for image in images]\n",
    "images = [image[0] for image in images]\n",
    "\n",
    "n = 10\n",
    "imgs_batched_by_n = [ # 50 batches of (n, 3, 224, 224)\n",
    "    torch.stack(images[i:i+n])\n",
    "    for i in range(0, len(images), n)\n",
    "] \n",
    "\n",
    "\n",
    "with torch.no_grad(): # Macbook air does not have fun here. Still only takes a minute and a half\n",
    "    test_image_features = np.concatenate(\n",
    "        [model.encode_image(img_batch).float() for img_batch in imgs_batched_by_n], axis=0\n",
    "    ) # (500, 1024)\n",
    "\n",
    "    test_image_features /= np.linalg.norm(test_image_features, axis=-1, keepdims=True) # I need the numpy version here because of my stacking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dotting the normalized image features with the normalized word vectors gives us the cosine similarity between the image and the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.2%\n"
     ]
    }
   ],
   "source": [
    "similarities = text_features.numpy() @ test_image_features.T\n",
    "\n",
    "likely_labels = np.argmax(similarities, axis=0)\n",
    "accuracy = sum(likely_labels == test_labels) / len(test_labels)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This substantially beats the baseline of random guessing, which is just guessing the most common class. This baseline is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common class in my random sample is 'automobile', which accounts for 65 of the 500 images.\n",
      "This means that a model that always predicts 'automobile' would have an accuracy of 13.0%.\n"
     ]
    }
   ],
   "source": [
    "mode = max(set(test_labels), key=test_labels.count)\n",
    "print(f\"The most common class in my random sample is '{cifar10_test.classes[mode]}', which accounts for {test_labels.count(mode)} of the 500 images.\")\n",
    "print(f\"This means that a model that always predicts '{cifar10_test.classes[mode]}' would have an accuracy of {test_labels.count(mode)/len(test_labels):.1%}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good baseline, but we can do better, even at 0-shot learning. Let us try some variants of the captions. We will pick a caption format based on performance on the CIFAR-10 training set, then see if it can beat our accuracy on the test set. First we get our train image embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = CIFAR10(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True, train=True)\n",
    "image_indicies = random.sample(range(len(cifar10_train)), 1000) # This comes to 1/5th of the train set\n",
    "images = [cifar10_train[idx] for idx in image_indicies] # 1000 tuples of (image, label) where image is \n",
    "\n",
    "train_labels = [image[1] for image in images]\n",
    "images = [image[0] for image in images]\n",
    "\n",
    "n = 10\n",
    "imgs_batched_by_n = [ # 50 batches of (n, 3, 224, 224)\n",
    "    torch.stack(images[i:i+n])\n",
    "    for i in range(0, len(images), n)\n",
    "] \n",
    "with torch.no_grad(): # Macbook air does not have fun here. Still only takes only 3 minutes\n",
    "    train_image_features = np.concatenate(\n",
    "        [model.encode_image(img_batch).float() for img_batch in imgs_batched_by_n], axis=0\n",
    "    ) # (500, 1024)\n",
    "\n",
    "    train_image_features /= np.linalg.norm(train_image_features, axis=-1, keepdims=True) # I need the numpy version here because of my stacking\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build a little helper function to get the accuracy of a given caption format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_format(prompt_format, image_embeddings, labels, classes):\n",
    "    descriptions = [prompt_format.replace(\"{class}\", label) for label in classes]\n",
    "    text_tokens = clip.tokenize(descriptions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarities = text_features.numpy() @ image_embeddings.T\n",
    "\n",
    "    likely_labels = np.argmax(similarities, axis=0)\n",
    "    accuracy = sum(likely_labels == labels) / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try some different caption formats: \n",
    "First, keeping it simple, what if we just use the name of the class with no effort to make a full sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 68.6%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on train set: {test_format('{class}', train_image_features, train_labels, cifar10_train.classes):.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not quite there, so here are some other ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\n",
    "    \"This is a photo of a {class}\",\n",
    "    \"This is a photo of a {class}.\",\n",
    "    \"The above image is a photo of a {class}.\",\n",
    "    \"This picture is a photo of a {class}.\",\n",
    "    \"Picture of a {class}.\",\n",
    "    \"This image is a photo of a {class}.\",\n",
    "    \"This is a {class}.\",\n",
    "    \"This is an image of a {class}\",\n",
    "    \"{class}\",\n",
    "    \"This is a {class} picture.\",\n",
    "    \"A {class}.\",\n",
    "    \"A {class}\"\n",
    "]\n",
    "training_accuracies = [test_format(format, train_image_features, train_labels, cifar10_train.classes) for format in formats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of our original caption format on the train set: 72.6%\n",
      "Using the format:   'This is a photo of a {class}.',\n",
      " we get a training accuracy of 71.5%\n",
      "\n",
      "Using the format:   'The above image is a photo of a {class}.',\n",
      " we get a training accuracy of 73.7%\n",
      "\n",
      "Using the format:   'This picture is a photo of a {class}.',\n",
      " we get a training accuracy of 74.4%\n",
      "\n",
      "Using the format:   'Picture of a {class}.',\n",
      " we get a training accuracy of 71.8%\n",
      "\n",
      "Using the format:   'This image is a photo of a {class}.',\n",
      " we get a training accuracy of 73.4%\n",
      "\n",
      "Using the format:   'This is a {class}.',\n",
      " we get a training accuracy of 69.0%\n",
      "\n",
      "Using the format:   'This is an image of a {class}',\n",
      " we get a training accuracy of 71.3%\n",
      "\n",
      "Using the format:   '{class}',\n",
      " we get a training accuracy of 68.6%\n",
      "\n",
      "Using the format:   'This is a {class} picture.',\n",
      " we get a training accuracy of 65.5%\n",
      "\n",
      "Using the format:   'A {class}.',\n",
      " we get a training accuracy of 71.6%\n",
      "\n",
      "Using the format:   'A {class}',\n",
      " we get a training accuracy of 70.4%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance of our original caption format on the train set: {training_accuracies[0]:.1%}\")\n",
    "\n",
    "for format, accuracy in zip(formats[1:], training_accuracies[1:]):\n",
    "    print(f\"Using the format:   '{format}',\\n we get a training accuracy of {accuracy:.1%}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we see that our original format is marginally outperformed by ```'This picture is a photo of a {class}.'``` So applying this to our testing data, we find that we get an accuracy of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 76.4%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on test set: {test_format('This picture is a photo of a {class}.', test_image_features, test_labels, cifar10_test.classes):.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed this does marginally beat our baseline. This is extremely interesting because adding a period to the end of the classes will likely effect the tokanization of the name of the class. Likely our class names are now being tokenized as more than one word, which I would not expect to increase performance. This is a very interesting result. I tried a couple other random seeds and I consistently get the same result.\n",
    "\n",
    "IMPORTANT NOTE: This prompt format that worked well for one of {Cifar-10, Cifar-100} did not work well for the other. I tried a couple different random seeds and consistently got the same result. I am not sure why this is the case, but it is interesting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Linear Probing\n",
    "\n",
    "Turns out those 1000 training images we already embedded will prove very useful! Let's use that to train up a little linear probe! In training, we often hit 100% accuracy, which implies that we should probably do closed form linear regression. I did have to make some interesting hyperparameter choices here though. I feel that using a gridseach to perfect my hyperparameter choices based on picking even more images out of the training to use as validation would be in some ways cheating. That is that it would allow for the hyperparamets on this random seed that just happen to work well on the validation set to be chosen, which will correspond to the test data. So instead I pick hyperparameters so that the classifier I build works well not only on Cifar-10 but also on Cifar-100, where working well implies a near monotonic rise in accuracy where not too many of the last epochs are done at perfect accuracy. I also believe that this data is realizable so I stick with the large batch size of 1/10th of my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 0.99      Training accuracy: 71.0%\n",
      "Epoch 1 training loss: 0.61      Training accuracy: 79.0%\n",
      "Epoch 2 training loss: 0.58      Training accuracy: 82.0%\n",
      "Epoch 3 training loss: 0.50      Training accuracy: 83.0%\n",
      "Epoch 4 training loss: 0.41      Training accuracy: 88.0%\n",
      "Epoch 5 training loss: 0.35      Training accuracy: 87.0%\n",
      "Epoch 6 training loss: 0.35      Training accuracy: 91.0%\n",
      "Epoch 7 training loss: 0.26      Training accuracy: 95.0%\n",
      "Epoch 8 training loss: 0.31      Training accuracy: 94.0%\n",
      "Epoch 9 training loss: 0.28      Training accuracy: 88.0%\n",
      "Epoch 10 training loss: 0.24      Training accuracy: 93.0%\n",
      "Epoch 11 training loss: 0.23      Training accuracy: 93.0%\n",
      "Epoch 12 training loss: 0.18      Training accuracy: 95.0%\n",
      "Epoch 13 training loss: 0.24      Training accuracy: 95.0%\n",
      "Epoch 14 training loss: 0.28      Training accuracy: 93.0%\n",
      "Epoch 15 training loss: 0.18      Training accuracy: 97.0%\n",
      "Epoch 16 training loss: 0.17      Training accuracy: 95.0%\n",
      "Epoch 17 training loss: 0.13      Training accuracy: 98.0%\n",
      "Epoch 18 training loss: 0.16      Training accuracy: 97.0%\n",
      "Epoch 19 training loss: 0.20      Training accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "# Building a dataloader from the train_image_features and train_labels\n",
    "\n",
    "training_dataset = torch.utils.data.TensorDataset(torch.tensor(train_image_features), torch.tensor(train_labels))\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "linear_probe = torch.nn.Linear(1024, 10)\n",
    "\n",
    "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = linear_probe(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} training loss: {loss.item():.2f}      Training accuracy: {sum(torch.argmax(logits, axis=1) == labels).item()/len(labels):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "labels_1hot = np.zeros((len(train_labels), 10))\n",
    "labels_1hot[np.arange(len(train_labels)), train_labels] = 1\n",
    "\n",
    "reg = LinearRegression().fit(train_image_features, labels_1hot)\n",
    "print(f\"Accuracy on train set: {sum(np.argmax(reg.predict(train_image_features), axis=1) == train_labels)/len(train_labels):.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the optimization of the linear probe is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of linear probe: 87.2%\n",
      "Accuracy of linear regression: 21.4%\n"
     ]
    }
   ],
   "source": [
    "linear_probe.eval()\n",
    "test_image_features_tensor = torch.tensor(test_image_features)\n",
    "probe_logits = linear_probe(test_image_features_tensor).detach().numpy()\n",
    "linear_model_accuracy = sum(np.argmax(probe_logits, axis=1) == test_labels)/len(test_labels)\n",
    "regression_accuracy = sum(np.argmax(reg.predict(test_image_features), axis=1) == test_labels)/len(test_labels)\n",
    "\n",
    "print(f\"Accuracy of linear probe: {linear_model_accuracy:.1%}\")\n",
    "print(f\"Accuracy of linear regression: {regression_accuracy:.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: 0-Shot Image Classification (CIFAR 100)\n",
    "Largely the same code as before, but with a different dataset. Interestingly, a couple different results.\n",
    "## Task 1\n",
    "\n",
    "Finding the image features is the first step. We will use the pretrained ResNet50 model to extract the features from 500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "cifar100_test = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True, train=False)\n",
    "\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100_test.classes]\n",
    "text_tokens = clip.tokenize(text_descriptions)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features.shape # (10, 1024) is for the 10 classes and 1024 dimensional embeddings\n",
    "\n",
    "image_indicies = random.sample(range(len(cifar100_test)), 500) # This comes to 1/20th of the test set\n",
    "images = [cifar100_test[idx] for idx in image_indicies] # 500 tuples of (image, label) where image is \n",
    "\n",
    "test_labels = [image[1] for image in images]\n",
    "images = [image[0] for image in images]\n",
    "\n",
    "n = 10\n",
    "imgs_batched_by_n = [ # 50 batches of (n, 3, 224, 224)\n",
    "    torch.stack(images[i:i+n])\n",
    "    for i in range(0, len(images), n)\n",
    "] \n",
    "\n",
    "\n",
    "with torch.no_grad(): # Macbook air does not have fun here. Still only takes a minute and a half\n",
    "    test_image_features = np.concatenate(\n",
    "        [model.encode_image(img_batch).float() for img_batch in imgs_batched_by_n], axis=0\n",
    "    ) # (500, 1024)\n",
    "\n",
    "    test_image_features /= np.linalg.norm(test_image_features, axis=-1, keepdims=True) # I need the numpy version here because of my stacking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dotting the normalized image features with the normalized word vectors gives us the cosine similarity between the image and the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.2%\n"
     ]
    }
   ],
   "source": [
    "similarities = text_features.numpy() @ test_image_features.T\n",
    "\n",
    "likely_labels = np.argmax(similarities, axis=0)\n",
    "accuracy = sum(likely_labels == test_labels) / len(test_labels)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This substantially beats the baseline of random guessing, which is just guessing the most common class. This baseline is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common class in my random sample is 'dinosaur', which accounts for 11 of the 500 images.\n",
      "This means that a model that always predicts 'dinosaur' would have an accuracy of 2.2%.\n"
     ]
    }
   ],
   "source": [
    "mode = max(set(test_labels), key=test_labels.count)\n",
    "print(f\"The most common class in my random sample is '{cifar100_test.classes[mode]}', which accounts for {test_labels.count(mode)} of the 500 images.\")\n",
    "print(f\"This means that a model that always predicts '{cifar100_test.classes[mode]}' would have an accuracy of {test_labels.count(mode)/len(test_labels):.1%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good baseline, but we can do better, even at 0-shot learning. Let us try some variants of the captions. We will pick a caption format based on performance on the CIFAR-10 training set, then see if it can beat our accuracy on the test set. First we get our train image embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar100_train = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True, train=True)\n",
    "image_indicies = random.sample(range(len(cifar100_train)), 1000) # This comes to 1/5th of the train set\n",
    "images = [cifar100_train[idx] for idx in image_indicies] # 1000 tuples of (image, label) where image is \n",
    "\n",
    "train_labels = [image[1] for image in images]\n",
    "images = [image[0] for image in images]\n",
    "\n",
    "n = 10\n",
    "imgs_batched_by_n = [ # 50 batches of (n, 3, 224, 224)\n",
    "    torch.stack(images[i:i+n])\n",
    "    for i in range(0, len(images), n)\n",
    "] \n",
    "with torch.no_grad(): # Macbook air does not have fun here. Still only takes only 3 minutes\n",
    "    train_image_features = np.concatenate(\n",
    "        [model.encode_image(img_batch).float() for img_batch in imgs_batched_by_n], axis=0\n",
    "    ) # (500, 1024)\n",
    "\n",
    "    train_image_features /= np.linalg.norm(train_image_features, axis=-1, keepdims=True) # I need the numpy version here because of my stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try some different caption formats: \n",
    "First, keeping it simple, what if we just use the name of the class with no effort to make a full sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 28.7%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on train set: {test_format('{class}', train_image_features, train_labels, cifar100_train.classes):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not quite there, so here are some other ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\n",
    "    \"This is a photo of a {class}\",\n",
    "    \"This is a photo of a {class}.\",\n",
    "    \"The above image is a photo of a {class}.\",\n",
    "    \"This picture is a photo of a {class}.\",\n",
    "    \"Picture of a {class}.\",\n",
    "    \"This image is a photo of a {class}.\",\n",
    "    \"This is a {class}.\",\n",
    "    \"This is an image of a {class}\",\n",
    "    \"{class}\",\n",
    "    \"This is a {class} picture.\",\n",
    "    \"A {class}.\",\n",
    "    \"A {class}\"\n",
    "]\n",
    "training_accuracies = [test_format(format, train_image_features, train_labels, cifar100_train.classes) for format in formats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of our original caption format on the train set: 40.1%\n",
      "Using the format:   'This is a photo of a {class}.',\n",
      " we get a training accuracy of 41.1%\n",
      "\n",
      "Using the format:   'The above image is a photo of a {class}.',\n",
      " we get a training accuracy of 41.0%\n",
      "\n",
      "Using the format:   'This picture is a photo of a {class}.',\n",
      " we get a training accuracy of 38.2%\n",
      "\n",
      "Using the format:   'Picture of a {class}.',\n",
      " we get a training accuracy of 40.9%\n",
      "\n",
      "Using the format:   'This image is a photo of a {class}.',\n",
      " we get a training accuracy of 38.5%\n",
      "\n",
      "Using the format:   'This is a {class}.',\n",
      " we get a training accuracy of 40.8%\n",
      "\n",
      "Using the format:   'This is an image of a {class}',\n",
      " we get a training accuracy of 41.3%\n",
      "\n",
      "Using the format:   '{class}',\n",
      " we get a training accuracy of 28.7%\n",
      "\n",
      "Using the format:   'This is a {class} picture.',\n",
      " we get a training accuracy of 33.9%\n",
      "\n",
      "Using the format:   'A {class}.',\n",
      " we get a training accuracy of 38.0%\n",
      "\n",
      "Using the format:   'A {class}',\n",
      " we get a training accuracy of 36.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance of our original caption format on the train set: {training_accuracies[0]:.1%}\")\n",
    "\n",
    "for format, accuracy in zip(formats[1:], training_accuracies[1:]):\n",
    "    print(f\"Using the format:   '{format}',\\n we get a training accuracy of {accuracy:.1%}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we see that our original format is marginally outperformed by ```'The above image is a photo of a {class}.'``` So applying this to our testing data, we find that we get an accuracy of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 37.6%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on test set: {test_format('The above image is a photo of a {class}.', test_image_features, test_labels, cifar100_test.classes):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed this does marginally beat our baseline. This is extremely interesting because adding a period to the end of the classes will likely effect the tokanization of the name of the class. Likely our class names are now being tokenized as more than one word, which I would not expect to increase performance. This is a very interesting result. I tried a couple other random seeds and I consistently get the same result.\n",
    "\n",
    "IMPORTANT NOTE: This prompt format that worked well for one of {Cifar-10, Cifar-100} did not work well for the other. I tried a couple different random seeds and consistently got the same result. I am not sure why this is the case, but it is interesting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Linear Probing\n",
    "\n",
    "Turns out those 1000 training images we already embedded will prove very useful! Let's use that to train up a little linear probe! In training, we often hit 100% accuracy, which implies that we should probably do closed form linear regression. I did have to make some interesting hyperparameter choices here though. I feel that using a gridseach to perfect my hyperparameter choices based on picking even more images out of the training to use as validation would be in some ways cheating. That is that it would allow for the hyperparamets on this random seed that just happen to work well on the validation set to be chosen, which will correspond to the test data. So instead I pick hyperparameters so that the classifier I build works well not only on Cifar-10 but also on Cifar-100, where working well implies a near monotonic rise in accuracy where not too many of the last epochs are done at perfect accuracy. I also believe that this data is realizable so I stick with the large batch size of 1/10th of my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 3.64      Training accuracy: 28.0%\n",
      "Epoch 1 training loss: 2.17      Training accuracy: 46.0%\n",
      "Epoch 2 training loss: 1.57      Training accuracy: 73.0%\n",
      "Epoch 3 training loss: 1.24      Training accuracy: 72.0%\n",
      "Epoch 4 training loss: 0.91      Training accuracy: 83.0%\n",
      "Epoch 5 training loss: 0.73      Training accuracy: 89.0%\n",
      "Epoch 6 training loss: 0.58      Training accuracy: 91.0%\n",
      "Epoch 7 training loss: 0.52      Training accuracy: 96.0%\n",
      "Epoch 8 training loss: 0.40      Training accuracy: 95.0%\n",
      "Epoch 9 training loss: 0.35      Training accuracy: 96.0%\n",
      "Epoch 10 training loss: 0.29      Training accuracy: 100.0%\n",
      "Epoch 11 training loss: 0.28      Training accuracy: 98.0%\n",
      "Epoch 12 training loss: 0.21      Training accuracy: 99.0%\n",
      "Epoch 13 training loss: 0.18      Training accuracy: 99.0%\n",
      "Epoch 14 training loss: 0.19      Training accuracy: 100.0%\n",
      "Epoch 15 training loss: 0.13      Training accuracy: 100.0%\n",
      "Epoch 16 training loss: 0.14      Training accuracy: 100.0%\n",
      "Epoch 17 training loss: 0.13      Training accuracy: 100.0%\n",
      "Epoch 18 training loss: 0.11      Training accuracy: 100.0%\n",
      "Epoch 19 training loss: 0.09      Training accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Building a dataloader from the train_image_features and train_labels\n",
    "\n",
    "training_dataset = torch.utils.data.TensorDataset(torch.tensor(train_image_features), torch.tensor(train_labels))\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "linear_probe = torch.nn.Linear(1024, 100)\n",
    "\n",
    "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = linear_probe(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} training loss: {loss.item():.2f}      Training accuracy: {sum(torch.argmax(logits, axis=1) == labels).item()/len(labels):.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "labels_1hot = np.zeros((len(train_labels), 100))\n",
    "labels_1hot[np.arange(len(train_labels)), train_labels] = 1\n",
    "\n",
    "reg = LinearRegression().fit(train_image_features, labels_1hot)\n",
    "print(f\"Accuracy on train set: {sum(np.argmax(reg.predict(train_image_features), axis=1) == train_labels)/len(train_labels):.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the optimization of the linear probe is convex, so we can just do closed form linear regression. This is a very simple operation but interestingly does not yield as good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of linear probe: 44.4%\n",
      "Accuracy of linear regression: 4.4%\n"
     ]
    }
   ],
   "source": [
    "linear_probe.eval()\n",
    "test_image_features_tensor = torch.tensor(test_image_features)\n",
    "probe_logits = linear_probe(test_image_features_tensor).detach().numpy()\n",
    "linear_model_accuracy = sum(np.argmax(probe_logits, axis=1) == test_labels)/len(test_labels)\n",
    "regression_accuracy = sum(np.argmax(reg.predict(test_image_features), axis=1) == test_labels)/len(test_labels)\n",
    "\n",
    "print(f\"Accuracy of linear probe: {linear_model_accuracy:.1%}\")\n",
    "print(f\"Accuracy of linear regression: {regression_accuracy:.1%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Socratic Method for Image Captioning\n",
    "\n",
    "Let's start by building a little infrastructure to load the Flickr Dataset, then we will proceed to consult Socrates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
